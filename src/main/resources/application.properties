spring.application.name=work-reflection-service
# ------------------------------
# Spring Shell configuration
spring.shell.command.history.enabled=true
spring.shell.command.script.enabled=true
spring.shell.command.stacktrace.enabled=true
spring.shell.script.enabled=true
# ------------------------------
# Chroma Vector Store connection properties
#spring.ai.vectorstore.chroma.client.key-token=<your access token (if configure)>
#spring.ai.vectorstore.chroma.client.username=<your username (if configure)>
#spring.ai.vectorstore.chroma.client.password=<your password (if configure)>
spring.ai.vectorstore.chroma.base-url=http://localhost:8000
spring.ai.vectorstore.chroma.collection-name=production-collection
spring.ai.vectorstore.chroma.initialize-schema=true
# ------------------------------
# OpenAI Config
#spring.ai.openai.api-key=${OPENAI_API_KEY_ENV}
#spring.ai.openai.chat.options.model=mistral
#spring.ai.openai.chat.base-url=http://localhost:11434
# ------------------------------
# Ollama properties
#spring.ai.ollama.model=llama3.2
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.embedding.options.model=mxbai-embed-large
# -------------------------------
# Default chat options for Ollama
spring.ai.ollama.chat.options.model=llama3.2
spring.ai.ollama.chat.options.temperature=0.7
spring.ai.ollama.chat.options.top-k=40
spring.ai.ollama.chat.options.top-p=0.9
# -------------------------------
# I/O
logging.level.org.springframework.ai.chat.client.advisor=DEBUG
output.filePath=reflections
# -------------------------------
# GITHUB
github.token=${GITHUB_TOKEN_REFLECTION_SERVICE}
